{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4444069d-d699-4326-bc9c-a7d1b56b532b",
   "metadata": {},
   "source": [
    "# Data Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f91d75-3dc2-43d8-81ef-b5181cbe1118",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5edd652-0677-4c83-8e37-23bb33876578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc0b40-7416-4902-a6c1-591ae7df4800",
   "metadata": {},
   "source": [
    "## Gestures List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29f4822-300c-4c55-a7d6-fbe7b01b5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = ['fist', 'palm', 'swing', 'vaishnavi']   # 4 gestures model will recognise\n",
    "test_data_dir = 'data/test'      \n",
    "train_data_dir = 'data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88eb355f-46b6-4bb7-8dd5-a827ea25b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Images = []\n",
    "no_train_images = 1000\n",
    "\n",
    "for gesture in gestures:\n",
    "    for i in range(0, no_train_images):\n",
    "        # print(train_data_dir + '/' + gesture + '/' + gesture + '_' + str(i) + '.png')\n",
    "        image = cv2.imread(train_data_dir + '/' + gesture + '/' + gesture + '_' + str(i) + '.png')\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        train_Images.append(gray_image.reshape(89, 100, 1)) #1 for grayscale(1 color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a947294d-6cb5-4ba4-a0e2-bee98f12ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Labels = []\n",
    "no_train_images = 1000\n",
    "\n",
    "for i in range(0, len(gestures)):\n",
    "    vect = [0] * len(gestures)\n",
    "    vect[i] = 1\n",
    "    for j in range(0, no_train_images):\n",
    "        train_Labels.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997c3f2f-18bb-4491-959f-9f2832b7c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Images = []\n",
    "no_test_images = 100\n",
    "\n",
    "for gesture in gestures:\n",
    "    for i in range(0, no_test_images):\n",
    "        # print(train_data_dir + '/' + gesture + '/' + gesture + '_' + str(i) + '.png')\n",
    "        image = cv2.imread(train_data_dir + '/' + gesture + '/' + gesture + '_' + str(i) + '.png')\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        test_Images.append(gray_image.reshape(89, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5732e974-11ee-4d8f-8ceb-bd613d4cac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Labels = []\n",
    "no_test_images = 100\n",
    "\n",
    "for i in range(0, len(gestures)):\n",
    "    vect = [0] * len(gestures)\n",
    "    vect[i] = 1\n",
    "    for j in range(0, no_test_images):\n",
    "        test_Labels.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a615f2-04b6-4348-bc2c-ca117a54cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images: 4000\n",
      "Training Labels: 4000\n",
      "Testing Images: 400\n",
      "Testing Labels: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Images: \" + str(len(train_Images)))\n",
    "print(\"Training Labels: \" + str(len(train_Labels)))\n",
    "print(\"Testing Images: \" + str(len(test_Images)))\n",
    "print(\"Testing Labels: \" + str(len(test_Labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d792a-5c59-4c98-aaf2-82e54a7ba90d",
   "metadata": {},
   "source": [
    "### Transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c713e2-24c5-45cd-8dee-713c68d229a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Images = np.array(train_Images)\n",
    "train_Labels = np.array(train_Labels)\n",
    "test_Images = np.array(test_Images)\n",
    "test_Labels = np.array(test_Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3562b4b-3563-4d5d-b780-273602712542",
   "metadata": {},
   "source": [
    "## Making CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4fbfac3-8905-418a-af32-68e0f775b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "model = models.Sequential([\n",
    "    #cnn layers\n",
    "    layers.Conv2D(filters = 32, activation='relu', kernel_size = (2,2), input_shape = (89,100,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(filters = 64, activation='relu', kernel_size = (2,2)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(filters = 128, activation='relu', kernel_size = (2,2)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(filters = 256, activation='relu', kernel_size = (2,2)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    #dense\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1000, activation = 'relu'),\n",
    "    layers.Dropout((0.40)),\n",
    "    layers.Dense(500, activation = 'relu'),\n",
    "    layers.Dropout((0.40)),\n",
    "    # layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(4, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a4a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 88, 99, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 44, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 43, 48, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 23, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 10, 256)        131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              5121000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 5,796,144\n",
      "Trainable params: 5,796,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4444b-bcfb-4327-ab89-c2ad41e1cd4c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9559f9f-1b7d-4068-88e3-f90d5608421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75086188-9b63-4c10-bbbf-34a328c1ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Training Data\n",
    "train_Images, train_Labels = shuffle(train_Images, train_Labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d16320c-8aa8-44da-8959-7def1cb6f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 42s 317ms/step - loss: 1.6228 - accuracy: 0.9408 - val_loss: 2.0859e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 38s 302ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 7.1992e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 38s 308ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 6.4034e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 38s 306ms/step - loss: 0.0208 - accuracy: 0.9958 - val_loss: 3.1743e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.0130 - accuracy: 0.9977 - val_loss: 7.3653e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_Images, train_Labels, epochs=100, validation_data = (test_Images, test_Labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932a6d0-85cc-4ccc-9c51-9ae0d4b86f01",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7beba01b-14a6-436e-b45a-be4a7015cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./TrainedModel/assets\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('TrainedModel'):\n",
    "    os.mkdir('TrainedModel')\n",
    "model.save('./TrainedModel/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd421d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
